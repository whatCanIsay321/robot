# from markdown_it.rules_block import heading
# from openai import OpenAI
# import json
# from utils.file_helper import MarkdownTokenizerTool
# # å¦‚æœä½ æ˜¯è‡ªå»º deepseek æœåŠ¡ / æœ¬åœ° vLLM / DashScope æ¥å£ï¼Œè¯·æ”¹æˆå¯¹åº” base_url
# client = OpenAI(
#     api_key="sk-dcb3caa7963645669e3205cae1f39464",
#     base_url="https://api.deepseek.com"
# )
#
#
# previous_structure=None
# # ========== 2. Markdown æ•°æ® ==========
# md_text = ''''''
#
# # ========== 3. Prompt æ„é€  ==========
# # âœ… ä½¿ç”¨ f-string æ¥æ’å…¥ md_text
# prompt = f"""
# ä½ æ˜¯ä¸€å Markdown æ–‡æ¡£ç»“æ„åˆ†æåŠ©æ‰‹ã€‚
#
# ã€ä»»åŠ¡ç›®æ ‡ã€‘
# æˆ‘ä¼šåˆ†æ®µæä¾› Markdown æ–‡æœ¬ï¼ˆcurrent_chunkï¼‰ï¼Œè¯·ä½ ä»æœ¬æ®µä¸­**æŠ½å–æ–°çš„æ ‡é¢˜å±‚çº§ç»“æ„**ï¼Œå¹¶æ„å»ºç›®å½•æ ‘ã€‚
# ä½ éœ€è¦ç»“åˆä¸Šä¸€æ®µçš„ç»“æ„ï¼ˆprevious_structureï¼‰æ¥ä¿æŒå±‚çº§è¿ç»­æ€§ï¼Œä½†ä¸èƒ½é‡å¤å‰ä¸€æ®µå·²æŠ½å–çš„å†…å®¹ã€‚
#
# ã€æŠ½å–è§„åˆ™ã€‘
# 1. åªå¤„ç†å½“å‰æ®µï¼ˆcurrent_chunkï¼‰ä¸­çš„æ–°å†…å®¹ï¼Œä¸è¦é‡å¤ previous_structure ä¸­å·²æœ‰çš„æ ‡é¢˜ã€‚
# 2. å¦‚æœå‘ç°æ ‡é¢˜å±‚çº§å»¶ç»­è‡ªä¸Šä¸€æ®µï¼ˆä¾‹å¦‚ä¸Šä¸€æ®µä»¥ "1.2" ç»“æŸï¼Œå½“å‰æ®µä» "1.2.1" å¼€å§‹ï¼‰ï¼Œåº”æ­£ç¡®åµŒå…¥å¯¹åº”çš„ä¸Šå±‚æ ‡é¢˜ä¸‹ã€‚
# 3. å¦‚æœæ–‡æœ¬ä¸­å­˜åœ¨â€œç›®å½•â€ã€â€œContentsâ€ã€â€œTable of Contentsâ€ç­‰éƒ¨åˆ†ï¼Œè¯·å°†è¯¥éƒ¨åˆ†**å•ç‹¬ä¿å­˜**åˆ° "detected_toc" å­—æ®µä¸­ï¼Œä¸å‚ä¸ç»“æ„æ ‘æ„å»ºã€‚
# 4. è¾“å‡ºç»“æœä¸­å¿…é¡»åŒ…å«ä¸¤ä¸ªéƒ¨åˆ†ï¼š
#    - "detected_toc"ï¼šä¿å­˜åŸæ–‡ä¸­ç›®å½•éƒ¨åˆ†çš„åŸå§‹æ–‡æœ¬ï¼›
#    - "new_structure"ï¼šå½“å‰æ®µæ–°å¢çš„æ ‡é¢˜å±‚çº§æ ‘ï¼ˆä¸åŒ…å«é‡å¤å†…å®¹ï¼‰ã€‚
# 5. æ¯ä¸ªèŠ‚ç‚¹éƒ½å¿…é¡»åŒ…å« "children" é”®ï¼Œå³ä½¿ä¸ºç©ºå¯¹è±¡ã€‚
# 6. æŒ‰æ ‡é¢˜åœ¨æ–‡ä¸­å‡ºç°çš„é¡ºåºæ„å»ºå±‚çº§ï¼Œä¸è¦é‡æ–°æ’åºã€‚
# 7. è¾“å‡ºå¿…é¡»ä¸º**ä¸¥æ ¼ JSON æ ¼å¼**ï¼Œä¸è¦é™„å¸¦è¯´æ˜ã€æ³¨é‡Šæˆ–å¤šä½™æ–‡æœ¬ã€‚
#
# ã€è¾“å‡ºæ ¼å¼ã€‘
# {{
#   "detected_toc": {{
#     "raw_text": "ç›®å½•åŸæ–‡æ–‡æœ¬ï¼ˆè‹¥æ— åˆ™ä¸º nullï¼‰"
#   }},
#   "new_structure": {{
#     "æ ‡é¢˜1": {{
#       "children": {{
#         "å­æ ‡é¢˜1": {{"children": {{}}}},
#         "å­æ ‡é¢˜2": {{"children": {{}}}}
#       }}
#     }}
#   }}
# }}
#
# ã€è¾“å…¥æ•°æ®ã€‘
# === ä¸Šä¸€æ®µå·²æŠ½å–çš„å±‚çº§ç»“æ„ ===
# {previous_structure}
#
# === å½“å‰æ®µ Markdown æ–‡æœ¬ ===
# {md_text}
#
# ã€è¾“å‡ºè¦æ±‚ã€‘
# - åªè¾“å‡ºåˆæ³• JSONã€‚
# - ä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–é¢å¤–æ–‡å­—ã€‚
# - JSON å¿…é¡»å¯ä»¥ç›´æ¥è§£æã€‚
# """
#
#
#
#
# # ========== 4. è°ƒç”¨ DeepSeek æ¨¡å‹ ==========
# def analyze_md_tree_with_deepseek(prompt: str):
#     response = client.chat.completions.create(
#         model="deepseek-chat",  # æˆ– deepseek-coder / DeepSeek-R1-Distill-Llama-70B
#         messages=[{"role": "user", "content": prompt}],
#         temperature=0
#     )
#
#     content = response.choices[0].message.content.strip()
#     try:
#         tree = json.loads(content)
#         return tree
#     except Exception:
#         print("âš ï¸ æ¨¡å‹è¾“å‡ºéçº¯JSONï¼Œè¯·æ‰‹åŠ¨æŸ¥çœ‹ï¼š\n", content)
#         return None
#
#
# # ========== 5. æ‰§è¡Œå¹¶æ‰“å°ç»“æœ ==========
# if __name__ == "__main__":
#     model_path = r"D:\PycharmProjects\robot\utils\tokenizer"
#
#     tool = MarkdownTokenizerTool(model_path)
#     cleaned_path = r'D:\PycharmProjects\robot\raw_data\è”ç»œä¸­å¿ƒåŠäº‹æŒ‡å—åŠå¸¸è§é—®é¢˜ç™¾é—®ç™¾ç­”_cleaned.md'
#     chunks = tool.chunk_until_token_limit(cleaned_path, max_tokens=2048)
#
#
#     result = analyze_md_tree_with_deepseek(prompt)
#     if result:
#         print(json.dumps(result, ensure_ascii=False, indent=2))
import os
import json
from copy import deepcopy
from openai import OpenAI
from utils.file_helper import MarkdownTokenizerTool


# ========== DeepSeek å®¢æˆ·ç«¯ ==========
client = OpenAI(
    api_key="sk-dcb3caa7963645669e3205cae1f39464",
    base_url="https://api.deepseek.com"
)


# =======================================================
# ğŸ§© ç»“æ„åˆå¹¶å·¥å…·
# =======================================================






# =======================================================
# ğŸ§  æ¨¡å‹è°ƒç”¨å‡½æ•°
# =======================================================
def analyze_md_tree_with_deepseek(previous_structure, md_text):
    """è°ƒç”¨ DeepSeek åˆ†æå½“å‰å—"""
    prompt = f"""ä½ æ˜¯ä¸€å Markdown æ–‡æ¡£ç»“æ„åˆ†æåŠ©æ‰‹ã€‚

ã€ä»»åŠ¡ç›®æ ‡ã€‘
æˆ‘ä¼šåˆ†æ®µæä¾› Markdown æ–‡æœ¬ï¼ˆcurrent_chunkï¼‰ï¼Œè¯·ä½ ä»æœ¬æ®µä¸­**æŠ½å–æ–°çš„æ ‡é¢˜å±‚çº§ç»“æ„**ï¼Œå¹¶æ„å»ºç›®å½•æ ‘ã€‚
ä½ éœ€è¦ç»“åˆä¸Šä¸€æ®µçš„ç»“æ„ï¼ˆprevious_structureï¼‰æ¥ä¿æŒå±‚çº§è¿ç»­æ€§ï¼Œä½†ä¸èƒ½é‡å¤å‰ä¸€æ®µå·²æŠ½å–çš„å†…å®¹ã€‚

ã€æŠ½å–è§„åˆ™ã€‘
1. åªå¤„ç†å½“å‰æ®µï¼ˆcurrent_chunkï¼‰ä¸­çš„æ–°å†…å®¹ï¼Œä¸è¦é‡å¤ previous_structure ä¸­å·²æœ‰çš„æ ‡é¢˜ã€‚
2. å¦‚æœå‘ç°æ ‡é¢˜å±‚çº§å»¶ç»­è‡ªä¸Šä¸€æ®µï¼ˆä¾‹å¦‚ä¸Šä¸€æ®µä»¥ "1.2" ç»“æŸï¼Œå½“å‰æ®µä» "1.2.1" å¼€å§‹ï¼‰ï¼Œåº”æ­£ç¡®åµŒå…¥å¯¹åº”çš„ä¸Šå±‚æ ‡é¢˜ä¸‹ã€‚
3. å¦‚æœæ–‡æœ¬ä¸­å­˜åœ¨â€œç›®å½•â€ã€â€œContentsâ€ã€â€œTable of Contentsâ€ç­‰éƒ¨åˆ†ï¼Œè¯·å°†è¯¥éƒ¨åˆ†**å•ç‹¬ä¿å­˜**åˆ° "detected_toc" å­—æ®µä¸­ï¼Œä¸å‚ä¸ç»“æ„æ ‘æ„å»ºã€‚
4. è¾“å‡ºç»“æœä¸­å¿…é¡»åŒ…å«ä¸¤ä¸ªéƒ¨åˆ†ï¼š
   - "detected_toc"ï¼šä¿å­˜åŸæ–‡ä¸­ç›®å½•éƒ¨åˆ†çš„åŸå§‹æ–‡æœ¬ï¼›
   - "new_structure"ï¼šå½“å‰æ®µæ–°å¢çš„æ ‡é¢˜å±‚çº§æ ‘ï¼ˆä¸åŒ…å«é‡å¤å†…å®¹ï¼‰ã€‚
5. æ¯ä¸ªèŠ‚ç‚¹éƒ½å¿…é¡»åŒ…å« "children" é”®ï¼Œå³ä½¿ä¸ºç©ºå¯¹è±¡ã€‚
6. æŒ‰æ ‡é¢˜åœ¨æ–‡ä¸­å‡ºç°çš„é¡ºåºæ„å»ºå±‚çº§ï¼Œä¸è¦é‡æ–°æ’åºã€‚
7. è¾“å‡ºå¿…é¡»ä¸º**ä¸¥æ ¼ JSON æ ¼å¼**ï¼Œä¸è¦é™„å¸¦è¯´æ˜ã€æ³¨é‡Šæˆ–å¤šä½™æ–‡æœ¬ã€‚

ã€è¾“å‡ºæ ¼å¼ã€‘
{{
  "detected_toc": {{
    "raw_text": "ç›®å½•åŸæ–‡æ–‡æœ¬ï¼ˆè‹¥æ— åˆ™ä¸º nullï¼‰"
  }},
  "new_structure": {{
    "æ ‡é¢˜1": {{
      "children": {{
        "å­æ ‡é¢˜1": {{"children": {{}}}},
        "å­æ ‡é¢˜2": {{"children": {{}}}}
      }}
    }}
  }}
}}

ã€è¾“å…¥æ•°æ®ã€‘
=== ä¸Šä¸€æ®µå·²æŠ½å–çš„å±‚çº§ç»“æ„ ===
{json.dumps(previous_structure, ensure_ascii=False) if previous_structure else "null"}

=== å½“å‰æ®µ Markdown æ–‡æœ¬ ===
{md_text}

ã€è¾“å‡ºè¦æ±‚ã€‘
- åªè¾“å‡ºåˆæ³• JSONã€‚
- ä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–é¢å¤–æ–‡å­—ã€‚
- JSON å¿…é¡»å¯ä»¥ç›´æ¥è§£æã€‚
"""

    response = client.chat.completions.create(
        model="deepseek-chat",
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )

    content = response.choices[0].message.content.strip()
    try:
        tree = json.loads(content)
        return tree
    except Exception:
        print("âš ï¸ æ¨¡å‹è¾“å‡ºéçº¯JSONï¼Œè¯·æ‰‹åŠ¨æŸ¥çœ‹ï¼š\n", content)
        return None


# =======================================================
# ğŸš€ ä¸»æµç¨‹ï¼šå¾ªç¯å¤„ç† + å®æ—¶åˆå¹¶
# =======================================================
if __name__ == "__main__":
    model_path = r"D:\PycharmProjects\robot\utils\tokenizer"
    cleaned_path = r"D:\PycharmProjects\robot\raw_data\è”ç»œä¸­å¿ƒåŠäº‹æŒ‡å—åŠå¸¸è§é—®é¢˜ç™¾é—®ç™¾ç­”_cleaned.md"

    tool = MarkdownTokenizerTool(model_path)
    chunks = tool.chunk_until_token_limit(cleaned_path, max_tokens=2048)

    merged_structure = None  # âœ… ä»ç©ºå¼€å§‹

    for idx, chunk in enumerate(chunks, 1):
        print(f"\n=== ğŸ§© æ­£åœ¨åˆ†æç¬¬ {idx}/{len(chunks)} å— ===")

        # è°ƒç”¨æ¨¡å‹ï¼ˆä¼ å…¥å½“å‰åˆå¹¶ç»“æ„ä½œä¸º previous_structureï¼‰
        result = analyze_md_tree_with_deepseek(merged_structure, chunk)
        if not result:
            print(f"âš ï¸ ç¬¬ {idx} å—æ¨¡å‹è¾“å‡ºå¼‚å¸¸ï¼Œè·³è¿‡ã€‚")
            continue

        # âœ… åˆå¹¶æœ¬è½®ç»“æ„
        if idx > 1:
            merged_structure = merge_two_structures(merged_structure, result)
        else:
            merged_structure = result

        # ä¿å­˜ä¸­é—´ç»“æœï¼ˆé˜²æ­¢å´©æºƒä¸¢å¤±ï¼‰
        with open("merged_progress.json", "w", encoding="utf-8") as f:
            json.dump(merged_structure, f, ensure_ascii=False, indent=2)

        print(f"âœ… å·²åˆå¹¶åˆ°ç¬¬ {idx} å—ï¼Œå½“å‰ç»“æ„å·²æ›´æ–°ã€‚")

    # è¾“å‡ºæœ€ç»ˆç»“æœ
    with open("final_merged_markdown_structure.json", "w", encoding="utf-8") as f:
        json.dump(merged_structure, f, ensure_ascii=False, indent=2)

    print("\nğŸ‰ å…¨éƒ¨å®Œæˆï¼Œæœ€ç»ˆåˆå¹¶ç»“æ„å·²ä¿å­˜åˆ° final_merged_markdown_structure.json âœ…")
