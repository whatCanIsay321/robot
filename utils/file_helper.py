import os
from typing import List, Union
from transformers import AutoTokenizer


class MarkdownTokenizerTool:
    """
    Markdown Êñá‰ª∂Â§ÑÁêÜ‰∏é Tokenizer ËÆ°ÁÆóÂ∑•ÂÖ∑
    """

    def __init__(self, model_name_or_path: str):
<<<<<<< HEAD
        if not os.path.exists(model_name_or_path):
            raise FileNotFoundError(f"Tokenizer path not found: {model_name_or_path}")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)

    # ===========================================================
    # üîπ ËØªÂèñ‰∏éÊ∏ÖÁêÜ
    # ===========================================================
    @staticmethod
    def read_md_remove_empty_lines(file_path: str) -> List[str]:
        """ËØªÂèñ Markdown Êñá‰ª∂Âπ∂ÂéªÈô§Á©∫Ë°å"""
=======
        """
        ÂàùÂßãÂåñ tokenizer
        :param model_name_or_path: Ê®°ÂûãÂêçÁß∞ÊàñÊú¨Âú∞ tokenizer Ë∑ØÂæÑ
        """
        if not os.path.exists(model_name_or_path):
            raise FileNotFoundError(f"Tokenizer path not found: {model_name_or_path}")

        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)

    @staticmethod
    def read_md_remove_empty_lines(file_path: str) -> List[str]:
        """
        ËØªÂèñ Markdown Êñá‰ª∂Âπ∂ÂéªÈô§Á©∫Ë°å„ÄÇ
        :param file_path: Markdown Êñá‰ª∂Ë∑ØÂæÑ
        :return: ÂéªÈô§Á©∫Ë°åÂêéÁöÑË°åÂàóË°®
        """
>>>>>>> b6517ef668d9a64ccbb8ca43ac3797774a835347
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"Markdown file not found: {file_path}")

        with open(file_path, "r", encoding="utf-8") as f:
            lines = f.readlines()

        cleaned_lines = [line.rstrip() for line in lines if line.strip()]
        return cleaned_lines

<<<<<<< HEAD
    def save_cleaned_md(self, file_path: str, output_path: str = None) -> str:
        """
        ÂéªÈô§Á©∫Ë°åÂπ∂‰øùÂ≠ò‰∏∫Êñ∞ Markdown Êñá‰ª∂„ÄÇ
        Ë∞ÉÁî® read_md_remove_empty_lines() ËøõË°åÊ∏ÖÁêÜ„ÄÇ
        """
        # ‚úÖ Ê≠£Á°ÆË∞ÉÁî®Ê∏ÖÁêÜÂáΩÊï∞
        cleaned_lines = self.read_md_remove_empty_lines(file_path)

        # ÊãºÊé•ÊñáÊú¨ÔºåÁ°Æ‰øùÊØèË°åÂêéÊúâÊç¢Ë°åÁ¨¶
        cleaned_text = "\n".join(cleaned_lines) + "\n"

        # Ëá™Âä®ÁîüÊàêËæìÂá∫Êñá‰ª∂Âêç
        if output_path is None:
            base, ext = os.path.splitext(file_path)
            output_path = base + "_cleaned" + ext

        # ÂÜôÂÖ•Êñá‰ª∂
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(cleaned_text)

        print(f"‚úÖ Â∑≤‰øùÂ≠òÂéªÁ©∫Ë°åÂêéÁöÑ MarkdownÔºö{output_path}")
        print(f"üìÑ ÂÖ± {len(cleaned_lines)} Ë°åÔºàÂ∑≤ÂéªÈô§Á©∫Ë°åÔºâ")
        return output_path

    # ===========================================================
    # üîπ Tokenizer Áõ∏ÂÖ≥
    # ===========================================================
    def count_tokens(self, text: Union[str, List[str]]) -> int:
        """ËÆ°ÁÆóÊñáÊú¨ÁöÑ token Êï∞"""
        if isinstance(text, list):
            text = "\n".join(text)
=======
    def count_tokens(self, text: Union[str, List[str]]) -> int:
        """
        ËÆ°ÁÆóÊñáÊú¨ÊàñÂ§öË°åÊñáÊú¨ÁöÑ token Êï∞Èáè
        :param text: ÊñáÊú¨Â≠óÁ¨¶‰∏≤ÊàñÂ≠óÁ¨¶‰∏≤ÂàóË°®
        :return: token Êï∞
        """
        if isinstance(text, list):
            text = "\n".join(text)

>>>>>>> b6517ef668d9a64ccbb8ca43ac3797774a835347
        tokens = self.tokenizer.encode(text, add_special_tokens=False)
        return len(tokens)

    def encode_text(self, text: str):
<<<<<<< HEAD
        return self.tokenizer([text], return_tensors="pt")

    # ===========================================================
    # üîπ ÂàÜÂùóÈÄªËæëÔºàÊîØÊåÅÂ§öÂùóËæìÂá∫Ôºâ
    # ===========================================================
    def chunk_until_token_limit(self, file_path: str, max_tokens: int = 2000) -> List[str]:
        """
        ÊåâË°åÈ°∫Â∫èÊãºÊé• Markdown ÊñáÊú¨ÔºåÁõ¥Âà∞ token Ë∂ÖÂá∫‰∏äÈôê‰∏∫Ê≠¢„ÄÇ
        Ë∂ÖÂá∫Êó∂Ëá™Âä®ÂºÄÂßãÊñ∞Âùó„ÄÇËã•ÂçïË°åË∂ÖËøá max_tokensÔºåÁõ¥Êé•Êä•Èîô„ÄÇ
        """
        # ‚úÖ Ë∞ÉÁî®ÂéªÁ©∫Ë°åÂêéÁöÑËØªÂèñÂáΩÊï∞
        lines = self.read_md_remove_empty_lines(file_path)

        chunks = []
        current_chunk = []
        current_tokens = 0

        for i, line in enumerate(lines, start=1):
            line_token_len = self.count_tokens(line)
            if line_token_len > max_tokens:
                raise ValueError(
                    f"‚ùå Á¨¨ {i} Ë°åË∂ÖÂá∫ÂçïÂùóÊúÄÂ§ß token ÈôêÂà∂ÔºÅ"
                    f" ÂΩìÂâçË°å {line_token_len} tokens > ÈôêÂà∂ {max_tokens}„ÄÇ\n"
                    f"Ë°åÂÜÖÂÆπÈ¢ÑËßàÔºö{line[:100]}..."
                )

            test_chunk = current_chunk + [line]
            token_len = self.count_tokens(test_chunk)

            if token_len > max_tokens:
                chunks.append("\n".join(current_chunk))
                print(f"üì¶ Â∑≤‰øùÂ≠òÁ¨¨ {len(chunks)} ÂùóÔºåÂÖ± {current_tokens} tokens„ÄÇ")
                current_chunk = [line]
                current_tokens = line_token_len
            else:
                current_chunk.append(line)
                current_tokens = token_len

        if current_chunk:
            chunks.append("\n".join(current_chunk))
            print(f"üì¶ Â∑≤‰øùÂ≠òÁ¨¨ {len(chunks)} ÂùóÔºåÂÖ± {current_tokens} tokens„ÄÇ")

        print(f"‚úÖ ÊÄªÂÖ±ÁîüÊàê {len(chunks)} ‰∏™ÂùóÔºàÊØèÂùó ‚â§ {max_tokens} tokensÔºâ")
        return chunks


if __name__ == "__main__":
    md_path = r"D:\PycharmProjects\robot\raw_data\ËÅîÁªú‰∏≠ÂøÉÂäû‰∫ãÊåáÂçóÂèäÂ∏∏ËßÅÈóÆÈ¢òÁôæÈóÆÁôæÁ≠î.md"
    model_path = r"./tokenizer"

    tool = MarkdownTokenizerTool(model_path)

    # 1Ô∏è‚É£ ÂéªÁ©∫Ë°åÂπ∂‰øùÂ≠ò
    cleaned_path = tool.save_cleaned_md(md_path)

    # 2Ô∏è‚É£ ÂàÜÂùó
    chunks = tool.chunk_until_token_limit(cleaned_path, max_tokens=2048)

=======
        """
        ËøîÂõû tokenizer ÁöÑÁºñÁ†ÅÁªìÊûúÔºàÂ¶Ç token idsÔºâ
        """
        return self.tokenizer([text], return_tensors="pt")

    def process_md_file(self, file_path: str, show_preview: bool = False) -> int:
        """
        ÁªºÂêàÊâßË°åÔºöËØªÂèñ Markdown ‚Üí ÂéªÁ©∫Ë°å ‚Üí ËÆ°ÁÆó token Êï∞
        :param file_path: Markdown Êñá‰ª∂Ë∑ØÂæÑ
        :param show_preview: ÊòØÂê¶ÊâìÂç∞ÈÉ®ÂàÜÂÜÖÂÆπÈ¢ÑËßà
        :return: token Êï∞
        """
        lines = self.read_md_remove_empty_lines(file_path)
        if show_preview:
            print("\n".join(lines[:10]))
            print("...")

        token_count = self.count_tokens(lines)
        print(f"üìò Êñá‰ª∂: {file_path}")
        print(f"üî¢ Token ÊÄªÊï∞: {token_count}")
        return token_count


if __name__ == "__main__":
    md_path = r"D:\PycharmProjects\robot\raw_data\ÂØåÂÆ∂Ê±áÂ∏∏ËßÅÈóÆÈ¢ò.md"
    model_path = r"./tokenizer"

    tool = MarkdownTokenizerTool(model_path)
    tool.process_md_file(md_path, show_preview=True)
>>>>>>> b6517ef668d9a64ccbb8ca43ac3797774a835347
