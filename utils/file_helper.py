import os
from typing import List, Union
from transformers import AutoTokenizer


class MarkdownTokenizerTool:
    """
    Markdown æ–‡ä»¶å¤„ç†ä¸ Tokenizer è®¡ç®—å·¥å…·
    """

    def __init__(self, model_name_or_path: str):
        if not os.path.exists(model_name_or_path):
            raise FileNotFoundError(f"Tokenizer path not found: {model_name_or_path}")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)

    # ===========================================================
    # ğŸ”¹ è¯»å–ä¸æ¸…ç†
    # ===========================================================
    @staticmethod
    def read_md_remove_empty_lines(file_path: str) -> List[str]:
        """è¯»å– Markdown æ–‡ä»¶å¹¶å»é™¤ç©ºè¡Œ"""
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"Markdown file not found: {file_path}")

        with open(file_path, "r", encoding="utf-8") as f:
            lines = f.readlines()

        cleaned_lines = [line.rstrip() for line in lines if line.strip()]
        return cleaned_lines

    def save_cleaned_md(self, file_path: str, output_path: str = None) -> str:
        """
        å»é™¤ç©ºè¡Œå¹¶ä¿å­˜ä¸ºæ–° Markdown æ–‡ä»¶ã€‚
        è°ƒç”¨ read_md_remove_empty_lines() è¿›è¡Œæ¸…ç†ã€‚
        """
        # âœ… æ­£ç¡®è°ƒç”¨æ¸…ç†å‡½æ•°
        cleaned_lines = self.read_md_remove_empty_lines(file_path)

        # æ‹¼æ¥æ–‡æœ¬ï¼Œç¡®ä¿æ¯è¡Œåæœ‰æ¢è¡Œç¬¦
        cleaned_text = "\n".join(cleaned_lines) + "\n"

        # è‡ªåŠ¨ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        if output_path is None:
            base, ext = os.path.splitext(file_path)
            output_path = base + "_cleaned" + ext

        # å†™å…¥æ–‡ä»¶
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(cleaned_text)

        print(f"âœ… å·²ä¿å­˜å»ç©ºè¡Œåçš„ Markdownï¼š{output_path}")
        print(f"ğŸ“„ å…± {len(cleaned_lines)} è¡Œï¼ˆå·²å»é™¤ç©ºè¡Œï¼‰")
        return output_path

    # ===========================================================
    # ğŸ”¹ Tokenizer ç›¸å…³
    # ===========================================================
    def count_tokens(self, text: Union[str, List[str]]) -> int:
        """è®¡ç®—æ–‡æœ¬çš„ token æ•°"""
        if isinstance(text, list):
            text = "\n".join(text)
        tokens = self.tokenizer.encode(text, add_special_tokens=False)
        return len(tokens)

    def encode_text(self, text: str):
        return self.tokenizer([text], return_tensors="pt")

    # ===========================================================
    # ğŸ”¹ åˆ†å—é€»è¾‘ï¼ˆæ”¯æŒå¤šå—è¾“å‡ºï¼‰
    # ===========================================================
    def chunk_until_token_limit(self, file_path: str, max_tokens: int = 2000) -> List[str]:
        """
        æŒ‰è¡Œé¡ºåºæ‹¼æ¥ Markdown æ–‡æœ¬ï¼Œç›´åˆ° token è¶…å‡ºä¸Šé™ä¸ºæ­¢ã€‚
        è¶…å‡ºæ—¶è‡ªåŠ¨å¼€å§‹æ–°å—ã€‚è‹¥å•è¡Œè¶…è¿‡ max_tokensï¼Œç›´æ¥æŠ¥é”™ã€‚
        """
        # âœ… è°ƒç”¨å»ç©ºè¡Œåçš„è¯»å–å‡½æ•°
        lines = self.read_md_remove_empty_lines(file_path)

        chunks = []
        current_chunk = []
        current_tokens = 0

        for i, line in enumerate(lines, start=1):
            line_token_len = self.count_tokens(line)
            if line_token_len > max_tokens:
                raise ValueError(
                    f"âŒ ç¬¬ {i} è¡Œè¶…å‡ºå•å—æœ€å¤§ token é™åˆ¶ï¼"
                    f" å½“å‰è¡Œ {line_token_len} tokens > é™åˆ¶ {max_tokens}ã€‚\n"
                    f"è¡Œå†…å®¹é¢„è§ˆï¼š{line[:100]}..."
                )

            test_chunk = current_chunk + [line]
            token_len = self.count_tokens(test_chunk)

            if token_len > max_tokens:
                chunks.append("\n".join(current_chunk))
                print(f"ğŸ“¦ å·²ä¿å­˜ç¬¬ {len(chunks)} å—ï¼Œå…± {current_tokens} tokensã€‚")
                current_chunk = [line]
                current_tokens = line_token_len
            else:
                current_chunk.append(line)
                current_tokens = token_len

        if current_chunk:
            chunks.append("\n".join(current_chunk))
            print(f"ğŸ“¦ å·²ä¿å­˜ç¬¬ {len(chunks)} å—ï¼Œå…± {current_tokens} tokensã€‚")

        print(f"âœ… æ€»å…±ç”Ÿæˆ {len(chunks)} ä¸ªå—ï¼ˆæ¯å— â‰¤ {max_tokens} tokensï¼‰")
        return chunks


if __name__ == "__main__":
    md_path = r"D:\PycharmProjects\robot\raw_data\è”ç»œä¸­å¿ƒåŠäº‹æŒ‡å—åŠå¸¸è§é—®é¢˜ç™¾é—®ç™¾ç­”.md"
    model_path = r"./tokenizer"

    tool = MarkdownTokenizerTool(model_path)

    # 1ï¸âƒ£ å»ç©ºè¡Œå¹¶ä¿å­˜
    cleaned_path = tool.save_cleaned_md(md_path)

    # 2ï¸âƒ£ åˆ†å—
    chunks = tool.chunk_until_token_limit(cleaned_path, max_tokens=2048)

