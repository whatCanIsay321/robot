import os
from typing import List, Union
from transformers import AutoTokenizer


class MarkdownTokenizerTool:
    """
    Markdown æ–‡ä»¶å¤„ç†ä¸ Tokenizer è®¡ç®—å·¥å…·
    """

    def __init__(self, model_name_or_path: str):
        """
        åˆå§‹åŒ– tokenizer
        :param model_name_or_path: æ¨¡å‹åç§°æˆ–æœ¬åœ° tokenizer è·¯å¾„
        """
        if not os.path.exists(model_name_or_path):
            raise FileNotFoundError(f"Tokenizer path not found: {model_name_or_path}")

        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)

    @staticmethod
    def read_md_remove_empty_lines(file_path: str) -> List[str]:
        """
        è¯»å– Markdown æ–‡ä»¶å¹¶å»é™¤ç©ºè¡Œã€‚
        :param file_path: Markdown æ–‡ä»¶è·¯å¾„
        :return: å»é™¤ç©ºè¡Œåçš„è¡Œåˆ—è¡¨
        """
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"Markdown file not found: {file_path}")

        with open(file_path, "r", encoding="utf-8") as f:
            lines = f.readlines()

        cleaned_lines = [line.rstrip() for line in lines if line.strip()]
        return cleaned_lines

    def count_tokens(self, text: Union[str, List[str]]) -> int:
        """
        è®¡ç®—æ–‡æœ¬æˆ–å¤šè¡Œæ–‡æœ¬çš„ token æ•°é‡
        :param text: æ–‡æœ¬å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨
        :return: token æ•°
        """
        if isinstance(text, list):
            text = "\n".join(text)

        tokens = self.tokenizer.encode(text, add_special_tokens=False)
        return len(tokens)

    def encode_text(self, text: str):
        """
        è¿”å› tokenizer çš„ç¼–ç ç»“æœï¼ˆå¦‚ token idsï¼‰
        """
        return self.tokenizer([text], return_tensors="pt")

    def process_md_file(self, file_path: str, show_preview: bool = False) -> int:
        """
        ç»¼åˆæ‰§è¡Œï¼šè¯»å– Markdown â†’ å»ç©ºè¡Œ â†’ è®¡ç®— token æ•°
        :param file_path: Markdown æ–‡ä»¶è·¯å¾„
        :param show_preview: æ˜¯å¦æ‰“å°éƒ¨åˆ†å†…å®¹é¢„è§ˆ
        :return: token æ•°
        """
        lines = self.read_md_remove_empty_lines(file_path)
        if show_preview:
            print("\n".join(lines[:10]))
            print("...")

        token_count = self.count_tokens(lines)
        print(f"ğŸ“˜ æ–‡ä»¶: {file_path}")
        print(f"ğŸ”¢ Token æ€»æ•°: {token_count}")
        return token_count


if __name__ == "__main__":
    md_path = r"D:\PycharmProjects\robot\raw_data\å¯Œå®¶æ±‡å¸¸è§é—®é¢˜.md"
    model_path = r"./tokenizer"

    tool = MarkdownTokenizerTool(model_path)
    tool.process_md_file(md_path, show_preview=True)
