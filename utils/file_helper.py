import os
from typing import List, Union
from transformers import AutoTokenizer


class MarkdownTokenizerTool:
    """
    Markdown æ–‡ä»¶å¤„ç†ä¸ Tokenizer è®¡ç®—å·¥å…·
    """

    # ===========================================================
    # ğŸ”¹ åˆå§‹åŒ–
    # ===========================================================
    def __init__(self, model_name_or_path: str):
        """
        åˆå§‹åŒ– tokenizer
        :param model_name_or_path: æ¨¡å‹åç§°æˆ–æœ¬åœ° tokenizer è·¯å¾„
        """
        if not os.path.exists(model_name_or_path):
            raise FileNotFoundError(f"Tokenizer path not found: {model_name_or_path}")

        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)

    # ===========================================================
    # ğŸ”¹ è¯»å–ä¸æ¸…ç†
    # ===========================================================
    @staticmethod
    def read_md_remove_empty_lines(file_path: str) -> List[str]:
        """
        è¯»å– Markdown æ–‡ä»¶å¹¶å»é™¤ç©ºè¡Œã€‚
        :param file_path: Markdown æ–‡ä»¶è·¯å¾„
        :return: å»é™¤ç©ºè¡Œåçš„è¡Œåˆ—è¡¨
        """
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"Markdown file not found: {file_path}")

        with open(file_path, "r", encoding="utf-8") as f:
            lines = f.readlines()

        cleaned_lines = [line.rstrip() for line in lines if line.strip()]
        return cleaned_lines

    def save_cleaned_md(self, file_path: str, output_path: str = None) -> str:
        """
        å»é™¤ç©ºè¡Œå¹¶ä¿å­˜ä¸ºæ–° Markdown æ–‡ä»¶ã€‚
        è°ƒç”¨ read_md_remove_empty_lines() è¿›è¡Œæ¸…ç†ã€‚
        """
        cleaned_lines = self.read_md_remove_empty_lines(file_path)
        cleaned_text = "\n".join(cleaned_lines) + "\n"

        # è‡ªåŠ¨ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        if output_path is None:
            base, ext = os.path.splitext(file_path)
            output_path = base + "_cleaned" + ext

        with open(output_path, "w", encoding="utf-8") as f:
            f.write(cleaned_text)

        print(f"âœ… å·²ä¿å­˜å»ç©ºè¡Œåçš„ Markdownï¼š{output_path}")
        print(f"ğŸ“„ å…± {len(cleaned_lines)} è¡Œï¼ˆå·²å»é™¤ç©ºè¡Œï¼‰")
        return output_path

    # ===========================================================
    # ğŸ”¹ Tokenizer ç›¸å…³
    # ===========================================================
    def count_tokens(self, text: Union[str, List[str]]) -> int:
        """
        è®¡ç®—æ–‡æœ¬çš„ token æ•°ã€‚
        :param text: æ–‡æœ¬å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨
        :return: token æ•°é‡
        """
        if isinstance(text, list):
            text = "\n".join(text)
        tokens = self.tokenizer.encode(text, add_special_tokens=False)
        return len(tokens)

    def encode_text(self, text: str):
        """è¿”å› tokenizer çš„ç¼–ç ç»“æœ"""
        return self.tokenizer([text], return_tensors="pt")

    # ===========================================================
    # ğŸ”¹ åˆ†å—é€»è¾‘ï¼ˆæ”¯æŒå¤šå—è¾“å‡ºï¼‰
    # ===========================================================
    def chunk_until_token_limit(self, file_path: str, max_tokens: int = 2000) -> List[str]:
        """
        æŒ‰è¡Œé¡ºåºæ‹¼æ¥ Markdown æ–‡æœ¬ï¼Œç›´åˆ° token è¶…å‡ºä¸Šé™ä¸ºæ­¢ã€‚
        è¶…å‡ºæ—¶è‡ªåŠ¨å¼€å§‹æ–°å—ã€‚è‹¥å•è¡Œè¶…è¿‡ max_tokensï¼Œç›´æ¥æŠ¥é”™ã€‚
        """
        lines = self.read_md_remove_empty_lines(file_path)

        chunks = []
        current_chunk = []
        current_tokens = 0

        for i, line in enumerate(lines, start=1):
            line_token_len = self.count_tokens(line)
            if line_token_len > max_tokens:
                raise ValueError(
                    f"âŒ ç¬¬ {i} è¡Œè¶…å‡ºå•å—æœ€å¤§ token é™åˆ¶ï¼"
                    f" å½“å‰è¡Œ {line_token_len} tokens > é™åˆ¶ {max_tokens}ã€‚\n"
                    f"è¡Œå†…å®¹é¢„è§ˆï¼š{line[:100]}..."
                )

            test_chunk = current_chunk + [line]
            token_len = self.count_tokens(test_chunk)

            if token_len > max_tokens:
                chunks.append("\n".join(current_chunk))
                print(f"ğŸ“¦ å·²ä¿å­˜ç¬¬ {len(chunks)} å—ï¼Œå…± {current_tokens} tokensã€‚")
                current_chunk = [line]
                current_tokens = line_token_len
            else:
                current_chunk.append(line)
                current_tokens = token_len

        if current_chunk:
            chunks.append("\n".join(current_chunk))
            print(f"ğŸ“¦ å·²ä¿å­˜ç¬¬ {len(chunks)} å—ï¼Œå…± {current_tokens} tokensã€‚")

        print(f"âœ… æ€»å…±ç”Ÿæˆ {len(chunks)} ä¸ªå—ï¼ˆæ¯å— â‰¤ {max_tokens} tokensï¼‰")
        return chunks


# ===========================================================
# ğŸ”¹ è¿è¡Œç¤ºä¾‹
# ===========================================================
if __name__ == "__main__":
    # md_path = r"D:\PycharmProjects\robot\raw_data\è”ç»œä¸­å¿ƒåŠäº‹æŒ‡å—åŠå¸¸è§é—®é¢˜ç™¾é—®ç™¾ç­”.md"
    # model_path = r"./tokenizer"
    #
    # tool = MarkdownTokenizerTool(model_path)
    #
    # # 1ï¸âƒ£ å»ç©ºè¡Œå¹¶ä¿å­˜
    # cleaned_path = tool.save_cleaned_md(md_path)

    # 2ï¸âƒ£ åˆ†å—
    # chunks = tool.chunk_until_token_limit(cleaned_path, max_tokens=2048)

    model_path = "./tokenizer"  # ä½ çš„ tokenizer ç›®å½•
    file_path = r"D:\PycharmProjects\robot\merged_progress_12.md"

    tool = MarkdownTokenizerTool(model_path)

    # è¯»å– Markdown å¹¶å»ç©ºè¡Œ
    lines = tool.read_md_remove_empty_lines(file_path)

    # è®¡ç®— token æ•°
    token_count = tool.count_tokens(lines)

    print(f"ğŸ”¢ æ–‡ä»¶å…± {token_count} ä¸ª tokens")