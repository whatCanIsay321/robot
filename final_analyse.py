import os
import json
from openai import OpenAI
from utils.file_helper import MarkdownTokenizerTool
from utils.extract_json import extract_single_json

# ===========================
# ğŸ”¹ åˆå§‹åŒ– DeepSeek å®¢æˆ·ç«¯
# ===========================
client = OpenAI(
    api_key="sk-dcb3caa7963645669e3205cae1f39464",
    base_url="https://api.deepseek.com"
)


# =======================================================
# ğŸ§  1ï¸âƒ£ æŠ½å–å‡½æ•°ï¼šä» Markdown å—ä¸­æå–å±‚çº§ç»“æ„
# =======================================================
def analyze_md_tree_with_deepseek(previous_structure, md_text):
    """è°ƒç”¨ DeepSeek æ¨¡å‹æŠ½å–å½“å‰å—çš„ç»“æ„"""

#     prompt = f'''ğŸ¤– Role
# ä½ æ˜¯ä¸€å Markdown æ–‡æ¡£ç»“æ„å¢é‡åˆ†æåŠ©æ‰‹ï¼Œè´Ÿè´£æ ¹æ®è¾“å…¥æ–‡æœ¬è‡ªåŠ¨æ›´æ–°æ–‡æ¡£çš„ç»“æ„ä¿¡æ¯ã€‚
# æ–‡æ¡£ç»“æ„ç”±ä¸¤ä¸ªéƒ¨åˆ†ç»„æˆï¼š
# 1. `ç›®å½•ç»“æ„ï¼ˆtocï¼‰`ï¼šä¿ç•™åŸæ–‡çš„ç›®å½•æ–‡æœ¬ï¼›
# 2. `æ ‡é¢˜å±‚çº§ç»“æ„ï¼ˆoutlineï¼‰`ï¼šç”± Markdown æ ‡é¢˜ï¼ˆ#ã€##ã€### â€¦ï¼‰ç»„æˆçš„å±‚çº§æ ‘ã€‚
#
# ---
#
# # ğŸ’¬ ä»»åŠ¡ç›®æ ‡
# æ ¹æ®è¾“å…¥ï¼š
# - `previous_structure`ï¼šä¸Šä¸€æ¬¡å¤„ç†åçš„ç»“æ„ï¼ˆåŒ…å« toc ä¸ outlineï¼‰ï¼›
# - `current_chunk`ï¼šå½“å‰æ–°å¢çš„ Markdown æ–‡æœ¬ï¼›
#
# åˆ†æå½“å‰å—ä¸­çš„å˜åŒ–ï¼Œå¹¶åœ¨ä¸ä¸¢å¤±æ—§æ•°æ®çš„å‰æä¸‹ï¼š
# - å¢é‡æ›´æ–°ç›®å½•å†…å®¹ï¼ˆè‹¥æ£€æµ‹åˆ°ç›®å½•ç« èŠ‚ï¼‰ï¼›
# - å¢é‡æ›´æ–°æ ‡é¢˜å±‚çº§ç»“æ„ï¼›
# - ä¿æŒå±‚çº§è¿ç»­ã€æ— é‡å¤ã€‚
#
# ---
#
# # âš™ï¸ è§„åˆ™
#
# ## 1ï¸âƒ£ ç›®å½•æ£€æµ‹ï¼ˆTOC æ›´æ–°ï¼‰
# - è‹¥æ£€æµ‹åˆ°â€œç›®å½•â€ã€â€œContentsâ€ã€â€œTable of Contentsâ€ç­‰å…³é”®è¯ï¼š
#   - æå–è¯¥éƒ¨åˆ†å®Œæ•´åŸæ–‡ä½œä¸ºæ–°çš„ç›®å½•å†…å®¹ï¼›
#   - è‹¥ä¹‹å‰å·²æœ‰ç›®å½•ï¼Œåˆ™è¿›è¡Œåˆå¹¶æˆ–æ›´æ–°ï¼ˆä¿ç•™å…¨éƒ¨ç‹¬ç«‹ç›®å½•æ–‡æœ¬ï¼‰ï¼›
#   - ç›®å½•ä¸å‚ä¸æ ‡é¢˜å±‚çº§ç»“æ„åˆ†æã€‚
#
# ## 2ï¸âƒ£ æ ‡é¢˜å±‚çº§æ›´æ–°ï¼ˆOutline æ›´æ–°ï¼‰
# - è¯†åˆ« Markdown æ ‡é¢˜ï¼ˆ# è‡³ ######ï¼‰ï¼›
# - ä»…æ–°å¢ `current_chunk` ä¸­çš„æ–°æ ‡é¢˜ï¼›
# - è‹¥æ ‡é¢˜å»¶ç»­ä¸Šå±‚ç»“æ„ï¼ˆä¾‹å¦‚ä¸Šä¸€éƒ¨åˆ†ä»¥ â€œ2.3â€ ç»“æŸï¼Œæœ¬æ®µä» â€œ2.3.1â€ å¼€å§‹ï¼‰ï¼Œ
#   è‡ªåŠ¨æŒ‚è½½åˆ°æ­£ç¡®çš„çˆ¶çº§ä¸‹ï¼›
# - æŒ‰æ–‡æœ¬å‡ºç°é¡ºåºæ„å»ºï¼Œä¸é‡æ’ã€ä¸ä¸¢å¤±ã€‚
#
# ## 3ï¸âƒ£ è¾“å‡ºè¦æ±‚
# - è¾“å‡º å®Œæ•´çš„æ›´æ–°åç»“æ„ï¼ŒåŒ…å«ä¸¤éƒ¨åˆ†ï¼š
#   1. `æ£€æµ‹åˆ°çš„ç›®å½•`ï¼ˆè‹¥æœ‰åˆ™ä¿ç•™åŸæ–‡ï¼›å¯åŒ…å«å¤šæ®µç›®å½•ï¼‰ï¼›
#   2. `åˆå¹¶åçš„æ ‡é¢˜å±‚çº§ç»“æ„`ã€‚
# - ä½¿ç”¨ Markdown å±‚çº§æ ¼å¼è¡¨ç¤ºç»“æ„ï¼š
#   ```markdown
#   # ä¸€çº§æ ‡é¢˜
#     - äºŒçº§æ ‡é¢˜
#       - ä¸‰çº§æ ‡é¢˜
#   ```
# - ä¸è¾“å‡º JSONã€ä¸è§£é‡Šã€ä¸æ·»åŠ è¯´æ˜ã€‚
#
# ---
#
# # ğŸ“¥ è¾“å…¥æ ¼å¼
# ```
# === previous_structure ===
# {previous_structure if previous_structure else "æ— "}
#
# === current_chunk ===
# {md_text}
# ```
#
# ---
#
# # ğŸ“¤ è¾“å‡ºæ ¼å¼
# ```
# æ£€æµ‹åˆ°çš„ç›®å½•ï¼š
# ï¼ˆè‹¥å­˜åœ¨ç›®å½•åˆ™ä¿ç•™åŸæ–‡ï¼‰
#
# # ä¸€çº§æ ‡é¢˜
#   - äºŒçº§æ ‡é¢˜
#     - ä¸‰çº§æ ‡é¢˜
# ```
#
# '''
    system='''You are a Long Document Structure Extraction Assistant. 
Your task is to incrementally build and update a hierarchical outline of a long document. 
Each time, the user will provide:
1. previous_structure: the previously extracted document structure.
2. new_content: a new portion of the document.

Your job:
1. Analyze how the new content relates to the existing structure.
2. Integrate it appropriately into the hierarchy.
3. Output the complete, updated document structure.

Output rules: 
- Use numbered hierarchical formatting (e.g., 1., 1.1) with a maximum of two levels.
- If the new content continues a previous section, add it as a subsection.
- If it starts a new section or chapter, create a new top-level node.
- Output only the final structure â€” no explanations, comments, or analysis.
- Keep the structure clear, consistent, and logically nested.

Language Requirement:
Please respond in Chinese, but you may include English technical terms when appropriate.

Output example:
1. Chapter Title
    1.1 Section Title
    1.2 Section Title
        1.2.1 Subsection Title
2. Next Chapter Title
    2.1 Section Title

    
'''
    system_nova = '''You are a Long Document Structure Extraction Assistant.  
Your task is to incrementally build and update a hierarchical outline of a long document.  
Each time, the user will provide:  
1. previous_structure: the previously extracted document structure.  
2. new_content: a new portion of the document.  

Your job:  
1. Analyze how the new content relates to the existing structure.  
2. Integrate it appropriately into the hierarchy, up to 2 levels.  
3. Use only the portion of new_content that forms complete structural units (e.g., full Chapters or Sections).  
   - If the ending cannot form a complete unit, treat it as unused content for future updates.  
4. Output the complete, updated document structure.  
5. At the end, include the first sentence of the unused content, formatted as:  
   [First sentence of unused content: "<first sentence>"]  

Output rules:  
- Use numbered hierarchical formatting (e.g., 1., 1.1, 1.1.1).  
- If the new content continues a previous section, add it as a subsection.  
- If it starts a new section or chapter, create a new top-level node.  
- Limit the structure depth to `{level_limit}` levels.  
- Output only the final structure and the â€œ[First sentence of unused content]â€ line â€” no explanations, comments, or analysis.  
- Keep the structure clear, consistent, and logically nested.  

Language Requirement:  
Please respond in Chinese, but you may include English technical terms when appropriate.  

Output example:  
1. Chapter Title  
    1.1 Section Title  
    1.2 Section Title  
2. Next Chapter Title  
[First sentence of unused content: "<first sentence>"]

    '''
    prompt = f'''=== previous_structure ===
{previous_structure if previous_structure else "null"}
=== current_chunk ===
{md_text}
    '''
    response = client.chat.completions.create(
        model="deepseek-chat",
        messages=[
            {"role": "system", "content": system},
            {"role": "user", "content": prompt}]
    )

    content = response.choices[0].message.content.strip()
    try:
        return content
    except Exception:
        print("âš ï¸ æ¨¡å‹è¾“å‡ºéçº¯JSONï¼Œè¯·æ‰‹åŠ¨æ£€æŸ¥ï¼š\n", content)
        return None

if __name__ == "__main__":
    model_path = r"./utils/tokenizer"
    cleaned_path = r"./raw_data/è”ç»œä¸­å¿ƒåŠäº‹æŒ‡å—åŠå¸¸è§é—®é¢˜ç™¾é—®ç™¾ç­”_cleaned.md"

    tool = MarkdownTokenizerTool(model_path)
    chunks = tool.chunk_until_token_limit(cleaned_path, max_tokens=2046)
    result = None  # åˆå§‹ä¸ºç©º

    for idx, chunk in enumerate(chunks, 1):
        print(f"\n=== ğŸ§© æ­£åœ¨åˆ†æç¬¬ {idx}/{len(chunks)} å— ===")
        result = analyze_md_tree_with_deepseek(result, chunk)
        with open(f"clean{idx}.md", "w", encoding="utf-8") as f:
            f.write(result)
        # # Step 3ï¸âƒ£: ä¿å­˜ä¸­é—´ç»“æœ
        # with open(f"gg_merged_progress_{idx}.md", "a", encoding="utf-8") as f:
        #     f.write(previous_structure + "\n")
        # with open(f"merged_progress_{idx}.json", "w", encoding="utf-8") as f:
        #     json.dump(previous_structure, f, ensure_ascii=False, indent=2)

        print(f"âœ… ç¬¬ {idx} å—å¤„ç†å®Œæˆï¼Œç»“æ„å·²æ›´æ–°ã€‚")

